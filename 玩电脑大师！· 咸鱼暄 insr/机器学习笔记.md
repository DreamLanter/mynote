- 李宏毅老师的课程。听的是 20 年的，课程主页（视频、slides 都有）：[http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html)
- 一个学长的笔记：[https://github.com/Sakura-gh/ML-notes](https://github.com/Sakura-gh/ML-notes)

- 并没有打算像其他文档那样很详细地记笔记qwq

- 这门课听了不困，这很稀有！


### 机器学习就是自动找函数

## Regression
Input: Information Output: A scalar
![image.png](./assets/1621672160270-3f7fb2c0-1ca8-47b3-8fc6-60fa80916467.png)

### Step 1. Model

#### Linear Model
![](https://cdn.nlark.com/yuque/__latex/11250c0c416cb76bf2e0202db5b2ee5f.svg#card=math&code=y%20%3D%20b%20%2B%20%5Csum%20w_ix_i&id=kdAod)。![](https://cdn.nlark.com/yuque/__latex/7a64f4ecd94889bec9719ba0dc1cf7bd.svg#card=math&code=b%5Ctext%7B%3A%20bias%2C%20%7Dw_i%5Ctext%7B%3A%20weight%2C%20%7Dx_i%5Ctext%7B%3A%20feature%7D&id=Ck1pV)


### Step 2. Goodness of Function
Loss Function - Input: a function, Output: how bad it is


### Step 3. Pick the Best Function


#### Gradient Descent | 梯度下降

- local optimal & global optimal (not in linear regression as the loss function is a convex function 凸函数)


### Overfitting

#### Regularization


### Where does the error come from
![image.png](./assets/1621674121899-04757dc0-d2ff-46ca-91db-320f6d384e00.png)
![image.png](./assets/1621674153161-6800f06e-6df8-4614-9261-9f8b8d94b66b.png)
![image.png](./assets/1621674166361-fd512cac-6262-4be7-b855-315bc4950774.png)

![image.png](./assets/1621674254576-ab05bc72-525a-4621-bdce-433759b46cc9.png)

### Gradient Descent +
还没学


## Classificaton
Input: Object; Output: Which class is the object in<br />找分布列


